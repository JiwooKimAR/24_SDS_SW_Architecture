{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWbpyZoGQqDC"
      },
      "source": [
        "# Lab 5. LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PJ8OkBVQqDE"
      },
      "source": [
        "- 단축키\n",
        "> Ctrl + Enter: 해당 셀 실행 <br>\n",
        "> Shift + Enter: 해당 셀 실행 후, 다음 셀로 이동 <br>\n",
        "> **셀 클릭 후,** <br>\n",
        "> A: 위 셀 생성 <br>\n",
        "> B: 아래 셀 생성 <br>\n",
        "> D + D: 셀 삭제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8UT6UHcQqDF"
      },
      "source": [
        "## Table of Contents\n",
        "- Environment setup\n",
        "- Document Summarization\n",
        "- Retrieval-Augmented Generation (RAG)\n",
        "- Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment setup"
      ],
      "metadata": {
        "id": "BdH3pRxYfpR3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fnG7mGoQqDF"
      },
      "outputs": [],
      "source": [
        "# API key가 제대로 생성되었는지 확인\n",
        "# 마지막 YOUR_API_KEY를 생성된 API_KEY로 바꾸어 확인\n",
        "!curl \\\n",
        "  -H 'Content-Type: application/json' \\\n",
        "  -d '{\"contents\":[{\"parts\":[{\"text\":\"Write a story about a magic backpack\"}]}]}' \\\n",
        "  -X POST https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=YOUR_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 packages를 설치\n",
        "!pip install langchain==0.1.9\n",
        "!pip install langchain_google_genai==0.0.9\n",
        "!pip install langchain-community==0.0.24\n",
        "!pip install langchainhub==0.1.15\n",
        "!pip install tiktoken==0.6.0\n",
        "!pip install chromadb==0.4.24"
      ],
      "metadata": {
        "id": "CJyXUsSzgpbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API_KEY 저장\n",
        "GOOGLE_API_KEY = \"YOUR_API_KEY\""
      ],
      "metadata": {
        "id": "DgQRhazOhBBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document Summarization"
      ],
      "metadata": {
        "id": "aXDV2pcUvJ2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "\n",
        "# 현재(24.02.29) 사용 가능 모델: 'models/text-bison-001'(요청 90/분), 'models/gemini-pro' (요청 60/분)\n",
        "llm = GoogleGenerativeAI(model='models/text-bison-001', google_api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "TeBFUdoas7oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarize a couple of sentences"
      ],
      "metadata": {
        "id": "OvTU_XbEZDyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "content = \"\"\"\n",
        "TEXT:\n",
        "The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\n",
        "It is characterized by its bold black-and-white coat and rotund body.\n",
        "The name \"giant panda\" is sometimes used to distinguish it from the red panda, a neighboring musteloid.\n",
        "Though it belongs to the order Carnivora, the giant panda is a folivore, with bamboo shoots and leaves making up more than 99% of its diet.\n",
        "Giant pandas in the wild occasionally eat other grasses, wild tubers, or even meat in the form of birds, rodents, or carrion.\n",
        "In captivity, they may receive honey, eggs, fish, yams, shrub leaves, oranges, or bananas along with specially prepared food.\"\"\""
      ],
      "metadata": {
        "id": "OpuSIjsBwglq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# string 프롬프트를 위한 템플릿(template)을 생성, 템플릿은 f-strings(default)나 jinja2로 포맷팅(formatting)이 가능\n",
        "\n",
        "# 1. PromptTemplate.from_template()을 사용하는 방법 (recommended)\n",
        "prompt = PromptTemplate.from_template(\"Please provide a summary of the following text in 2 lines. {content}\")\n",
        "\n",
        "# 2. PromptTemplate 생성자를 사용하는 방법\n",
        "# prompt = PromptTemplate(\n",
        "#     input_variables=[\"content\"],\n",
        "#     template=\"Please provide a summary of the following text in 2 lines. {content}\"\n",
        "# )"
      ],
      "metadata": {
        "id": "KRYL-meEzLx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM chain을 설정\n",
        "\n",
        "# 1. LLMChain 방법을 사용\n",
        "# llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "# llm_chain.invoke({'content': content})\n",
        "\n",
        "# 2. LCEL을 사용 (recommended)\n",
        "llm_chain = prompt | llm\n",
        "llm_chain.invoke({'content': content})"
      ],
      "metadata": {
        "id": "9u52tMs9yV5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarize a document"
      ],
      "metadata": {
        "id": "eLLwCXy5ZHsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "VD30vlwJZy2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "id": "5KUF3RUTZ0mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Write a concise summary of the following:\n",
        "\"{context}\"\n",
        "CONCISE SUMMARY:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(prompt_template)"
      ],
      "metadata": {
        "id": "bdIvPjJUaUUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = GoogleGenerativeAI(model='models/text-bison-001',\n",
        "                         google_api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "gCdedQYYaYGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
        "\n",
        "chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
        "chain.invoke({'context': docs})"
      ],
      "metadata": {
        "id": "DudsO_Qza-t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "**None 부분을 채워주세요!**"
      ],
      "metadata": {
        "id": "8Dy7100OcOsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
        "\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
        "docs = loader.load()\n",
        "\n",
        "prompt_template = None\n",
        "\n",
        "prompt = None\n",
        "\n",
        "llm = GoogleGenerativeAI(model='models/text-bison-001',\n",
        "                         google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "chain = None\n",
        "chain.invoke(None)"
      ],
      "metadata": {
        "id": "kw5Gxue_4ZyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 정답코드\n",
        "\n",
        "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
        "\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
        "docs = loader.load()\n",
        "\n",
        "prompt_template = \"\"\"Summarize the following blog post like a news article title and news abstract:\n",
        "\"{context}\"\n",
        "News title and abstract:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "llm = GoogleGenerativeAI(model='models/text-bison-001',\n",
        "                         google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
        "chain.invoke({'context': docs})"
      ],
      "metadata": {
        "id": "muwGFNoBcRyi",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "**None 부분을 채워주세요!**"
      ],
      "metadata": {
        "id": "pInTWKBidhtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = WebBaseLoader(\"https://www.sciencealert.com/one-of-the-fastest-meteor-showers-of-the-year-is-peaking-this-weekend-look-up\")\n",
        "docs = loader.load()\n",
        "\n",
        "prompt_template = None\n",
        "\n",
        "prompt = None\n",
        "\n",
        "llm = GoogleGenerativeAI(model='models/text-bison-001',\n",
        "                         google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "chain = None\n",
        "chain.invoke(None)"
      ],
      "metadata": {
        "id": "lyQpS_rVdjh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 정답코드\n",
        "\n",
        "loader = WebBaseLoader(\"https://www.sciencealert.com/one-of-the-fastest-meteor-showers-of-the-year-is-peaking-this-weekend-look-up\")\n",
        "docs = loader.load()\n",
        "\n",
        "prompt_template = \"\"\"Summarize the following news article in a few sentences and extract the hashtags:\n",
        "\"{context}\"\n",
        "Summarization and Hashtags:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "llm = GoogleGenerativeAI(model='models/text-bison-001',\n",
        "                         google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
        "chain.invoke({'context': docs})"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Tae4PVaP4n7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval-Augmented Generation (RAG)"
      ],
      "metadata": {
        "id": "9GuFxVaV5HSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similarity Search"
      ],
      "metadata": {
        "id": "5xWt4dye5cJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings"
      ],
      "metadata": {
        "id": "pSjufdzlnzX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 웹페이지(문서)를 로드\n",
        "loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",), bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))),)\n",
        "\n",
        "raw_documents = loader.load()"
      ],
      "metadata": {
        "id": "jdOlvbVOxztB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문서를 청크로 분리\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "  chunk_size=1000, # Maximum size of chunks to return\n",
        "  chunk_overlap=200 # Overlap in characters between chunks\n",
        ")\n",
        "splits = text_splitter.split_documents(raw_documents)"
      ],
      "metadata": {
        "id": "HZMD3lS07EQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 청크를 임베딩하고 이를 벡터 스토어에 저장\n",
        "vectorstore = Chroma.from_documents(\n",
        "  documents=splits,\n",
        "  embedding=GoogleGenerativeAIEmbeddings(model='models/embedding-001', google_api_key=GOOGLE_API_KEY)\n",
        ")"
      ],
      "metadata": {
        "id": "GBFPiDKM7HRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 벡터 스토어에 저장되어 있는 딕셔너리 키 값을 출력\n",
        "print(vectorstore.get().keys())"
      ],
      "metadata": {
        "id": "bEbEBnJoHwJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# 특정 ids의 값을 출력\n",
        "id = vectorstore.get()[\"ids\"][0]\n",
        "print(f'ID: {id}')\n",
        "pprint(vectorstore.get(ids=id))"
      ],
      "metadata": {
        "id": "BuKzueTKIg8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 벡터 스토어에 저장되어 있는 문서(청크) 두 개를 출력\n",
        "for i in range(2):\n",
        "  print(f'** Chunk {i} **')\n",
        "  print(vectorstore.get()['documents'][i])\n",
        "  print()"
      ],
      "metadata": {
        "id": "ddBNYI_tJObh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the types of memory?\"\n",
        "\n",
        "docs = vectorstore.similarity_search(query)\n",
        "\n",
        "print(\"** Document #1 **\")\n",
        "print(docs[0].page_content)\n",
        "\n",
        "print(\"\\n** Document #2 **\")\n",
        "print(docs[1].page_content)"
      ],
      "metadata": {
        "id": "3J8KEKh7KUv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the types of memory?\"\n",
        "query_embedding = GoogleGenerativeAIEmbeddings(model='models/embedding-001', google_api_key=GOOGLE_API_KEY).embed_query(query)\n",
        "\n",
        "docs = vectorstore.similarity_search_by_vector(query_embedding)\n",
        "\n",
        "print(\"** Document #1 **\")\n",
        "print(docs[0].page_content)\n",
        "\n",
        "print(\"\\n** Document #2 **\")\n",
        "print(docs[1].page_content)"
      ],
      "metadata": {
        "id": "YzQAj_0yMliG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise\n",
        "**None 부분을 채워주세요!**"
      ],
      "metadata": {
        "id": "ORT8gZXheSa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 위 셀에서 생성한 vectorstore 삭제\n",
        "vectorstore.delete_collection()"
      ],
      "metadata": {
        "id": "OKNRRRHbfp9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = WebBaseLoader(\"https://www.sciencealert.com/one-of-the-fastest-meteor-showers-of-the-year-is-peaking-this-weekend-look-up\")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = None\n",
        "splits = None\n",
        "\n",
        "vectorstore = None"
      ],
      "metadata": {
        "id": "zZSdNp0reXWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the Leonid meteor shower?\"\n",
        "# query = \"When is this event occur?\"\n",
        "retrieved_docs = None\n",
        "\n",
        "print(retrieved_docs[0].page_content)"
      ],
      "metadata": {
        "id": "DXVo0pQ_gn-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 정답코드\n",
        "loader = WebBaseLoader(\"https://www.sciencealert.com/one-of-the-fastest-meteor-showers-of-the-year-is-peaking-this-weekend-look-up\")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "  chunk_size=400, # Maximum size of chunks to return\n",
        "  chunk_overlap=100 # Overlap in characters between chunks\n",
        ")\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "  documents=splits,\n",
        "  embedding=GoogleGenerativeAIEmbeddings(model='models/embedding-001', google_api_key=GOOGLE_API_KEY)\n",
        ")\n",
        "\n",
        "query = \"What is the Leonid meteor shower?\"\n",
        "# query = \"When is this event occur?\"\n",
        "retrieved_docs = vectorstore.similarity_search(query)\n",
        "\n",
        "print(retrieved_docs[0].page_content)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oM62o_Kv49_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web-based Retrieval"
      ],
      "metadata": {
        "id": "89C3SS29xr0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "from langchain import hub\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain_google_genai import GoogleGenerativeAI"
      ],
      "metadata": {
        "id": "-MvSIzUPxwdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 위 셀에서 생성한 vectorstore 삭제\n",
        "vectorstore.delete_collection()"
      ],
      "metadata": {
        "id": "P9sSqTVaPb6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 웹페이지(문서)를 로드\n",
        "loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",), bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))),)\n",
        "raw_documents = loader.load()\n",
        "\n",
        "# 문서를 청크로 분리\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "  chunk_size=1000, # Maximum size of chunks to return\n",
        "  chunk_overlap=200 # Overlap in characters between chunks\n",
        ")\n",
        "splits = text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# 각 청크를 임베딩하고 이를 벡터 스토어에 저장\n",
        "vectorstore = Chroma.from_documents(\n",
        "  documents=splits,\n",
        "  embedding=GoogleGenerativeAIEmbeddings(model='models/embedding-001', google_api_key=GOOGLE_API_KEY)\n",
        ")"
      ],
      "metadata": {
        "id": "30IwaarEPCjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 리트리버 정의\n",
        "retriever = vectorstore.as_retriever(\n",
        "  search_type=\"similarity\",\n",
        "  search_kwargs={\"k\": 6}\n",
        ")\n",
        "\n",
        "retrieved_docs = retriever.get_relevant_documents(\n",
        "  \"What is the MIPS?\"\n",
        ")"
      ],
      "metadata": {
        "id": "q5w0sFvOP_pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 리트리브 된 문서의 개수를 출력\n",
        "print(len(retrieved_docs))\n",
        "print()\n",
        "\n",
        "# 첫 리트리브된 문서의 문서 내용을 출력\n",
        "print(retrieved_docs[0].page_content)"
      ],
      "metadata": {
        "id": "vrYaztddQPBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM 모델을 정의\n",
        "llm = GoogleGenerativeAI(model='models/gemini-pro', google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# 사전 정의된 프롬프트를 가져오기\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")"
      ],
      "metadata": {
        "id": "F8Vf6pQpRZtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사전 정의된 프롬프트 내용 확인\n",
        "# https://smith.langchain.com/hub/rlm/rag-prompt\n",
        "hub.pull(\"rlm/rag-prompt\")"
      ],
      "metadata": {
        "id": "t59u4stzO1ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 여러 개의 문서가 있을 때 이를 string으로 이어주는 함수\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# 체인 생성\n",
        "rag_chain = (\n",
        "  {\"context\": retriever | format_docs,\n",
        "   \"question\": RunnablePassthrough()}\n",
        "  | prompt\n",
        "  | llm\n",
        "  | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "t4nE9e2rTzXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain"
      ],
      "metadata": {
        "id": "jHuQRemCUL-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in rag_chain.stream(\"What is the MIPS?\"):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "u_a_imS6VFmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stream과 Invoke 비교"
      ],
      "metadata": {
        "id": "CmVCjpuQ8a2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stream\n",
        "for chunk in rag_chain.stream(\"What is the MIPS?\"):\n",
        "    print(chunk, end=\"|\", flush=True)"
      ],
      "metadata": {
        "id": "b7BNZhDK8ZxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# invoke\n",
        "rag_chain.invoke(\"What is the MIPS?\")"
      ],
      "metadata": {
        "id": "YE6b1weu8lEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise\n",
        "**None 부분을 채워주세요!**"
      ],
      "metadata": {
        "id": "GaoCAsOokkN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 위 셀에서 생성한 vectorstore 삭제\n",
        "vectorstore.delete_collection()"
      ],
      "metadata": {
        "id": "kuwNBT1_lD9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "# Indexing\n",
        "loader = WebBaseLoader(\"https://www.sciencealert.com/one-of-the-fastest-meteor-showers-of-the-year-is-peaking-this-weekend-look-up\")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = None\n",
        "splits = None\n",
        "\n",
        "vectorstore = None"
      ],
      "metadata": {
        "id": "9NUwLasAkmPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve\n",
        "# search type: similarity, search_kwargs: {\"k\": 2}\n",
        "retriever = None\n",
        "\n",
        "# LLM 모델을 정의\n",
        "llm = GoogleGenerativeAI(model='models/gemini-pro', google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# 사전 정의된 프롬프트를 가져오기\n",
        "prompt = None\n",
        "\n",
        "# 여러 개의 문서가 있을 때 이를 string으로 이어주는 함수\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ],
      "metadata": {
        "id": "kI1_yakalIbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 체인 생성\n",
        "rag_chain = None"
      ],
      "metadata": {
        "id": "E66IUfrXlOIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in rag_chain.stream(\"What is the Leonid meteor shower?\"):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "x_jowxgRlPjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in rag_chain.stream(\"When is this event occur?\"):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "7LYdXsFkld69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 정답코드\n",
        "\n",
        "from langchain import hub\n",
        "\n",
        "# Indexing\n",
        "loader = WebBaseLoader(\"https://www.sciencealert.com/one-of-the-fastest-meteor-showers-of-the-year-is-peaking-this-weekend-look-up\")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "  chunk_size=400, # Maximum size of chunks to return\n",
        "  chunk_overlap=100 # Overlap in characters between chunks\n",
        ")\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "  documents=splits,\n",
        "  embedding=GoogleGenerativeAIEmbeddings(model='models/embedding-001', google_api_key=GOOGLE_API_KEY)\n",
        ")\n",
        "\n",
        "# Retrieve\n",
        "retriever = vectorstore.as_retriever(\n",
        "  search_type=\"similarity\",\n",
        "  search_kwargs={\"k\": 2}\n",
        ")\n",
        "\n",
        "# LLM 모델을 정의\n",
        "llm = GoogleGenerativeAI(model='models/gemini-pro', google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# 사전 정의된 프롬프트를 가져오기\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# 여러 개의 문서가 있을 때 이를 string으로 이어주는 함수\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# 체인 생성\n",
        "rag_chain = (\n",
        "  {\"context\": retriever | format_docs,\n",
        "   \"question\": RunnablePassthrough()}\n",
        "  | prompt\n",
        "  | llm\n",
        "  | StrOutputParser()\n",
        ")\n",
        "\n",
        "for chunk in rag_chain.stream(\"What is the Leonid meteor shower?\"):\n",
        "    print(chunk, end=\"\", flush=True)\n",
        "\n",
        "# for chunk in rag_chain.stream(\"When is this event occur?\"):\n",
        "#   print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sucbnfoE5L4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot"
      ],
      "metadata": {
        "id": "_93uuWVB4__H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Memory"
      ],
      "metadata": {
        "id": "54JcpLYipbAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "memory = ChatMessageHistory()\n",
        "\n",
        "memory.add_user_message(\"hi!\")\n",
        "memory.add_ai_message(\"whats up?\")\n",
        "\n",
        "print(memory.messages)"
      ],
      "metadata": {
        "id": "nyGqXp-EnBb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant.\",\n",
        "        ),\n",
        "        HumanMessage(content=\"What is 3+5?\"),\n",
        "        AIMessage(content=\"It is 8.\"),\n",
        "        HumanMessage(content=\"What is 4x3?\"),\n",
        "        AIMessage(content=\"It is 12.\"),\n",
        "        HumanMessage(content=\"Add above equations.\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 간단한 체인 정의\n",
        "llm = GoogleGenerativeAI(model='models/gemini-pro', google_api_key=GOOGLE_API_KEY)\n",
        "chain = prompt | llm\n",
        "\n",
        "chain.invoke({'input': ''})"
      ],
      "metadata": {
        "id": "G0gTFQhFyvr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | llm\n",
        "chain.invoke({'messages': [\n",
        "    HumanMessage(content=\"What is 3+5?\"),\n",
        "    AIMessage(content=\"It is 8.\"),\n",
        "    HumanMessage(content=\"What is 4x3?\"),\n",
        "    AIMessage(content=\"It is 12.\"),\n",
        "    HumanMessage(content=\"Add above equations.\")\n",
        "]})"
      ],
      "metadata": {
        "id": "at5DpHKz0fDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "memory = ChatMessageHistory()\n",
        "\n",
        "memory.add_user_message(\"What is 3+5?\")\n",
        "memory.add_ai_message(\"It is 8.\")\n",
        "memory.add_user_message(\"What is 4x3?\")\n",
        "memory.add_ai_message(\"It is 12.\")\n",
        "memory.add_user_message(\"Add above equations.\")\n",
        "\n",
        "chain = prompt | llm\n",
        "response = chain.invoke({'messages': memory.messages})\n",
        "memory.add_ai_message(response)\n",
        "\n",
        "print(response)\n",
        "print(memory.messages)"
      ],
      "metadata": {
        "id": "7JQiWrua1i7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "**None 부분을 채워주세요!**"
      ],
      "metadata": {
        "id": "cZEujLnm33Ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ChatMessageHistory\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are helpful assistant. And you always say 'Ho-Ho-Ho.' after you answer my question.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "memory = None\n",
        "\n",
        "llm = GoogleGenerativeAI(model='models/gemini-pro', google_api_key=GOOGLE_API_KEY)\n",
        "chain = None"
      ],
      "metadata": {
        "id": "8SPh86zuyMsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  input_ = input(\"User: \")\n",
        "  if input_ == 'quit':\n",
        "    print(\"\\n**Message history**\")\n",
        "    for m in memory.messages:\n",
        "      print(m.content)\n",
        "    break\n",
        "  # memory에 user의 message를 추가\n",
        "  None\n",
        "  response = None\n",
        "  # memory에 ai의 message를 추가\n",
        "  None\n",
        "  print(response)"
      ],
      "metadata": {
        "id": "kvzpzMn1yUMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 정답코드\n",
        "\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ChatMessageHistory\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are helpful assistant. And you always say 'Ho-Ho-Ho.' after you answer my question.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "memory = ChatMessageHistory()\n",
        "\n",
        "llm = GoogleGenerativeAI(model='models/gemini-pro', google_api_key=GOOGLE_API_KEY)\n",
        "chain = prompt | llm\n",
        "\n",
        "while True:\n",
        "  input_ = input(\"User: \")\n",
        "  if input_ == 'quit':\n",
        "    print(\"\\n**Message history**\")\n",
        "    for m in memory.messages:\n",
        "      print(m.content)\n",
        "    break\n",
        "  memory.add_user_message(input_)\n",
        "  response = chain.invoke({'messages': memory.messages})\n",
        "  memory.add_ai_message(response)\n",
        "  print(response)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "azPUAZMw5_G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieval"
      ],
      "metadata": {
        "id": "RW4wNKsD3jsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 위 셀에서 생성한 vectorstore 삭제\n",
        "vectorstore.delete_collection()"
      ],
      "metadata": {
        "id": "v9GsqdSM6Ny1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 웹페이지(문서)를 로드\n",
        "loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",), bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))),)\n",
        "raw_documents = loader.load()\n",
        "\n",
        "# 문서를 청크로 분리\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "  chunk_size=1000, # Maximum size of chunks to return\n",
        "  chunk_overlap=200 # Overlap in characters between chunks\n",
        ")\n",
        "splits = text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# 각 청크를 임베딩하고 이를 벡터 스토어에 저장\n",
        "vectorstore = Chroma.from_documents(\n",
        "  documents=splits,\n",
        "  embedding=GoogleGenerativeAIEmbeddings(model='models/embedding-001', google_api_key=GOOGLE_API_KEY)\n",
        ")\n",
        "\n",
        "# 문서를 검색\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "docs = retriever.invoke(\"What are the types of memory?\")\n",
        "\n",
        "for i in range(len(docs)):\n",
        "  print(docs[i])"
      ],
      "metadata": {
        "id": "2-s6avXd5OCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "\n",
        "SYSTEM_TEMPLATE = \"\"\"\n",
        "Answer the user's questions based on the below context.\n",
        "If the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\":\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            SYSTEM_TEMPLATE,\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "llm = GoogleGenerativeAI(model='models/gemini-pro', google_api_key=GOOGLE_API_KEY)\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)"
      ],
      "metadata": {
        "id": "NSsWolHE3knI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "memory = ChatMessageHistory()\n",
        "memory.add_user_message(\"Explain about short-term memory.\")\n",
        "\n",
        "document_chain.invoke(\n",
        "    {\n",
        "        \"context\": docs,\n",
        "        \"messages\": memory.messages,\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "-IHGrp_T6jf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "memory = ChatMessageHistory()\n",
        "memory.add_user_message(\"Explain about short-term memory.\")\n",
        "\n",
        "document_chain.invoke(\n",
        "    {\n",
        "        \"context\": [], # 검색 결과를 추가하지 않은 경우\n",
        "        \"messages\": memory.messages,\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "0INw-Bzm7Fyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "**None 부분을 채워주세요!**"
      ],
      "metadata": {
        "id": "aXXgEgUb9SRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 위 셀에서 생성한 vectorstore 삭제\n",
        "vectorstore.delete_collection()"
      ],
      "metadata": {
        "id": "EhLy-z9M_z-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retriever와 LLM chain에서 사용됨\n",
        "HUMAN_MESSAGE = \"Tell me about iPhone 16.\"\n",
        "\n",
        "# Load Webpage\n",
        "loader = WebBaseLoader(\"https://www.macrumors.com/2024/03/20/iphone-16-ultra-thin-bezels-rumor/\")\n",
        "data = loader.load()\n",
        "\n",
        "# Split the text into chunks\n",
        "text_splitter = None\n",
        "all_splits = None"
      ],
      "metadata": {
        "id": "ShsDHm_-9UzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vector store\n",
        "vectorstore = None\n",
        "\n",
        "# Initialize LLM and retriever, k=20\n",
        "retriever = None"
      ],
      "metadata": {
        "id": "0A18S6jbBdcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "\n",
        "SYSTEM_TEMPLATE = \"\"\"\n",
        "Answer the user's questions based on the below context.\n",
        "If the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\":\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            SYSTEM_TEMPLATE,\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "llm = GoogleGenerativeAI(model='models/gemini-pro', google_api_key=GOOGLE_API_KEY)\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)"
      ],
      "metadata": {
        "id": "x3nQAsZo_SUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "memory = ChatMessageHistory()\n",
        "memory.add_user_message(None)\n",
        "\n",
        "document_chain.invoke(\n",
        "    {\n",
        "        \"context\": docs,\n",
        "        \"messages\": memory.messages,\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "wL4WprWY_VKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 정답코드\n",
        "\n",
        "# Retriever와 LLM chain에서 사용됨\n",
        "HUMAN_MESSAGE = \"Tell me about iPhone 16.\"\n",
        "\n",
        "# Load Webpage\n",
        "loader = WebBaseLoader(\"https://www.macrumors.com/2024/03/20/iphone-16-ultra-thin-bezels-rumor/\")\n",
        "data = loader.load()\n",
        "\n",
        "# Split the text into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "  chunk_size=1000,\n",
        "  chunk_overlap=200\n",
        ")\n",
        "all_splits = text_splitter.split_documents(data)\n",
        "\n",
        "# Create vector store\n",
        "vectorstore = Chroma.from_documents(\n",
        "  documents=all_splits,\n",
        "  embedding=GoogleGenerativeAIEmbeddings(model='models/embedding-001', google_api_key=GOOGLE_API_KEY)\n",
        ")\n",
        "\n",
        "# Initialize LLM and retriever\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
        "# Search documents\n",
        "docs = retriever.invoke(HUMAN_MESSAGE)\n",
        "\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "\n",
        "SYSTEM_TEMPLATE = \"\"\"\n",
        "Answer the user's questions based on the below context.\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            SYSTEM_TEMPLATE,\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "llm = GoogleGenerativeAI(model='models/gemini-pro', google_api_key=GOOGLE_API_KEY)\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "memory = ChatMessageHistory()\n",
        "memory.add_user_message(HUMAN_MESSAGE)\n",
        "\n",
        "document_chain.invoke(\n",
        "    {\n",
        "        \"context\": docs,\n",
        "        \"messages\": memory.messages,\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GgDPIdWWAQTK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}